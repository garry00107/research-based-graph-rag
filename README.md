# Research Paper Assistant ğŸ¤–ğŸ“š

An AI-powered research assistant that helps you find, read, and understand ArXiv research papers. Built with **FastAPI**, **LlamaIndex**, **NVIDIA NIM**, and **Next.js**.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![TypeScript](https://img.shields.io/badge/typescript-5.0+-blue.svg)
![Docker](https://img.shields.io/badge/docker-ready-blue.svg)

## ğŸš€ Features

### Core Capabilities
- **ğŸ” Smart Paper Search**: Search ArXiv by title, author, or keywords.
- **ğŸ¯ Advanced Filters**: Filter search results by category (e.g., AI, CV) and publication year.
- **ğŸ“¥ Batch Ingestion**: Select multiple papers and ingest them simultaneously.
- **âš¡ Async Processing**: Non-blocking ingestion using Celery background workers.
- **ğŸ’¬ AI Chat**: Ask questions about the papers with citation-backed answers.
- **ğŸŒŠ Streaming Responses**: Real-time token-by-token responses for a smooth experience.

### Advanced Features
- **ğŸ’¾ Vector Database**: Uses **ChromaDB** for fast, persistent vector storage.
- **ğŸš€ Redis Caching**: Caches embeddings and chat history for performance.
- **ğŸ“š Papers Library**: View, manage, and search your ingested papers.
- **ğŸ• Chat History**: Persists conversation history for context-aware answers.
- **ğŸ³ Docker Support**: Full stack deployment with a single command.

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI, Celery, Redis
- **AI Engine**: LlamaIndex, NVIDIA NIM (Llama 3.2 3B, NV-EmbedQA)
- **Vector Store**: ChromaDB
- **Frontend**: Next.js 14, TailwindCSS, Shadcn/UI
- **Infrastructure**: Docker Compose

## ğŸ Quick Start

### Prerequisites
- Docker & Docker Compose
- NVIDIA API Key (from [build.nvidia.com](https://build.nvidia.com))

### 1. Clone & Configure
```bash
git clone https://github.com/yourusername/research-paper-assistant.git
cd research-paper-assistant

# Create .env file
cp .env.example .env
# Edit .env and add your NVIDIA_API_KEY
```

### 2. Run with Docker
```bash
docker-compose up --build
```
Access the app at **http://localhost:3000**

## ğŸ‘¨â€ğŸ’» Local Development

### Backend
```bash
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Start Redis (Required)
redis-server

# Start Celery Worker (for async ingestion)
./start-celery.sh

# Start Backend
python main.py
```

### Frontend
```bash
cd frontend
npm install
npm run dev
```

## ğŸ“– Usage Guide

1. **Search Papers**: Click the âš™ï¸ icon to open the Admin Panel. Search for papers (e.g., "RAG agents").
2. **Ingest**: Select papers and click "Ingest Selected".
3. **Chat**: Ask questions like "What are the key findings of these papers?".
4. **Library**: Click the ğŸ“š icon to view your ingested papers.
5. **History**: Click the ğŸ• icon to view past conversations.

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py           # FastAPI endpoints
â”‚   â”œâ”€â”€ rag_engine.py     # RAG logic with ChromaDB
â”‚   â”œâ”€â”€ ingestion.py      # ArXiv search & PDF processing
â”‚   â”œâ”€â”€ cache.py          # Redis caching layer
â”‚   â”œâ”€â”€ celery_app.py     # Async task configuration
â”‚   â”œâ”€â”€ papers_library.py # Library management
â”‚   â””â”€â”€ chat_history.py   # Conversation memory
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/              # Next.js pages
â”‚   â”œâ”€â”€ components/       # React components (Chat, Library, History)
â”‚   â””â”€â”€ lib/api.ts        # API client
â””â”€â”€ docker-compose.yml    # Deployment config
```

## ğŸ¤ Contributing
Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“„ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
